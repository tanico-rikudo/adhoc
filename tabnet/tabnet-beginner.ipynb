{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0edf7c2d5f0f70628fc224588ceca2c2113a9ef071d19ac3a0b9346fc3eba554c",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/marcusgawronsky/tabnet-in-tensorflow-2-0/execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ichi.pro/pytorch-de-no-tabnet-no-jisso-277727554318969\n",
    "# https://zenn.dev/sinchir0/articles/9228eccebfbf579bfdf4\n",
    "# https://www.guruguru.science/competitions/16/discussions/70f25f95-4dcc-4733-9f9e-f7bc6472d7c0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ichi.pro/glu-ge-totsuki-senkei-yunitto-37276504891521\n",
    "# https://www.slideshare.net/JiroNishitoba/20170629a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.guruguru.science/competitions/16/discussions/70f25f95-4dcc-4733-9f9e-f7bc6472d7c0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tesorflow>=2.4\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "source": [
    "## GatedLinearUnit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    GLU:\n",
    "    Ghost batch normalizationを適用\n",
    "\n",
    "    GBNは汎化誤差の抑制する\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                n_a: Optional[int]=None,\n",
    "                n_d: Optional[int]=None,\n",
    "                virtual_batch_size: Optional[int]=128,\n",
    "                momentum: Optional[float]=0.01,\n",
    "                fc: tf.keras.layers.Layer = None,\n",
    "                apply_glu: bool = True,\n",
    "                ):\n",
    "            super(GLU, self).__init__()\n",
    "            self.units = 2*(n_a + n_d) \n",
    "            self.virtual_batch_size = virtual_batch_size\n",
    "            self.momentum = momentum\n",
    "    \n",
    "\n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "\n",
    "        self.fc_out = tf.keras.layers.Dense(self.units, use_bias=False) if fc is None else fc\n",
    "        self.bn_out = tf.keras.layers.BatchNormalization(\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum)\n",
    "\n",
    "    def call(self,\n",
    "        inputs: Union[tf.Tensor, np.ndarray], \n",
    "        training: Optional[bool]=None):\n",
    "        # Pass GBN\n",
    "        output=self.bn_out(self.fc_out(inputs), training=training)\n",
    "        if self.apply_glu:\n",
    "            # ゲート付き線形ユニット\n",
    "            return output[:,:(n_a + n_d)] * tf.keras.activations.sigmoid(output[:,(n_a + n_d):])\n",
    "        else\n",
    "            # そのまま通過\n",
    "            return output[:,:(n_a + n_d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                units: Optional[int] = None,\n",
    "                n_total: int = 4,\n",
    "                n_shared: int = 2,\n",
    "                virtual_batch_size: Optional[int]  = 128,\n",
    "                momentum: Optional[float]=0.02,\n",
    "                skip= False):\n",
    "        super(FeatureTransformer).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        self.n_a = n_a\n",
    "        self.n_d = n_d\n",
    "        self.n_share = n_share\n",
    "        self.n_decision = n_decision\n",
    "        self.share_fcs=List[tf.keras.layers.Layer] = [] # Share phase dence\n",
    "        self.skip = skip \n",
    "        self.blocks = []\n",
    "        # n_a -> to Attention\n",
    "        # n_d -> to final Dicision \n",
    "    \n",
    "    def  build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "\n",
    "        for n in range(n_share+n_decision):\n",
    "            # shared blocks\n",
    "            if self.share_fcs and n < len(fcs):\n",
    "                self.blocks.append(\n",
    "                    GLU(n_a=n_a, n_d=n_d,fc=self.share_fcs[n]\n",
    "                        virtual_batch_size=self.virtual_batch_size, momentum=self.momentum)\n",
    "            # build new blocks\n",
    "            else:\n",
    "                self.blocks.append(\n",
    "                    GLU(n_a=n_a, n_d=n_d\n",
    "                        virtual_batch_size=self.virtual_batch_size, momentum=self.momentum)\n",
    "\n",
    "    def call(self, \n",
    "            inputs: Union[tf.Tensor, np.ndarray], \n",
    "            training: Optional[bool] = None):\n",
    "\n",
    "        initial = self.initial(inputs, training=training)\n",
    "        \n",
    "        #　直前入力ある場合(true)は加算\n",
    "        #　何も無い場合(false)は初期化状態のママ\n",
    "        if self.skip == True:\n",
    "            initial += inputs\n",
    "\n",
    "        residual = self.residual(initial, training=training) \n",
    "        return (initial + residual) * np.sqrt(0.5)\n",
    "        \n",
    "    def call(self,\n",
    "             x: tf.Tensor, \n",
    "             training: Optional[bool] = None):\n",
    "        x = self.blocks[0](x, training=training)\n",
    "        for n in range(1, self.n_total):\n",
    "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(tf.keras.layers.Layer): \n",
    "    def __init__(self, \n",
    "                units: Optional[int] = None, \n",
    "                virtual_batch_size: Optional[int] = 128, \n",
    "                momentum: Optional[float] = 0.02):\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(\n",
    "            self.units, \n",
    "            use_bias=False)\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum)\n",
    "        \n",
    "    def call(self, \n",
    "            inputs: Union[tf.Tensor, np.ndarray], \n",
    "            priors: Optional[Union[tf.Tensor, np.ndarray]] = None, \n",
    "            training: Optional[bool] = None) -> tf.Tensor:\n",
    "        feature = self.bn(self.fc(inputs), \n",
    "                          training=training)\n",
    "        if priors is None:\n",
    "            output = feature\n",
    "        else:\n",
    "            output = feature * priors\n",
    "        \n",
    "        # sparsemax: softmaxの進化版\n",
    "        # return key( inject into musk)\n",
    "        return tfa.activations.sparsemax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                units: Optional[int] = None, \n",
    "                virtual_batch_size: Optional[int]=128, \n",
    "                momentum: Optional[float] =0.02):\n",
    "        super(TabNetStep, self).__init__()\n",
    "        self.units = units\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, \n",
    "            input_shape: tf.TensorShape):\n",
    "        if self.units is None:\n",
    "            self.units = input_shape[-1]\n",
    "        \n",
    "        self.unique = FeatureTransformer(\n",
    "            units = self.units, \n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum,\n",
    "            skip=True)\n",
    "        self.attention = AttentiveTransformer(\n",
    "            units = input_shape[-1], \n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum)\n",
    "\n",
    "    def call(self, \n",
    "            inputs, # input (raw)\n",
    "            shared, # shared ()\n",
    "            priors, #  prior\n",
    "            training=None) -> Tuple[tf.Tensor]:  \n",
    "\n",
    "        \n",
    "        split = self.unique(shared, training=training)\n",
    "\n",
    "        # mul attention\n",
    "        # 過去の情報を適用しMasked_xを生成\n",
    "        keys = self.attention(split, priors, training=training)\n",
    "        masked = keys * inputs\n",
    "        # split\n",
    "        # masked: \n",
    "        # keys: \n",
    "        return split, masked, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                units: int =1,\n",
    "                n_feature: int = 8,\n",
    "                n_steps: int = 3,\n",
    "                outputs: int = 1,\n",
    "                gammna: float = 1.3,\n",
    "                eps: float = 1e-8,\n",
    "                sparse: float = 1e-5,\n",
    "                virtual_batch_size: Optional[int] = 128,\n",
    "                momemtum: Optional[float] = 0.02 ):\n",
    "        super(TabNetEncoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.n_features = n_features\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.sparse = sparse\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "    def build(self, \n",
    "            input_shape: tf.TensorShape):\n",
    "        self.bn = tf.keras.layers.BatchNormalization(\n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum)\n",
    "        self.shared_block = FeatureTransformer(\n",
    "            units = self.n_features, \n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum)        \n",
    "        self.initial_step = TabNetStep(\n",
    "            units = self.n_features, \n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum)\n",
    "\n",
    "        self.steps = [\n",
    "            TabNetStep(\n",
    "                units = self.n_features,\n",
    "                virtual_batch_size=self.virtual_batch_size, \n",
    "                momentum=self.momentum) for _ in range(self.n_steps)]\n",
    "\n",
    "        self.final = tf.keras.layers.Dense(\n",
    "            units = self.units, \n",
    "            use_bias=False)\n",
    "\n",
    "    def call(self, \n",
    "            X: Union[tf.Tensor, np.ndarray], \n",
    "            training: Optional[bool] = None) -> Tuple[tf.Tensor]:    \n",
    "        entropy_loss = 0.0\n",
    "        encoded = 0.0\n",
    "        output = 0.0\n",
    "        importance = 0.0\n",
    "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
    "        \n",
    "        B = prior * self.bn(X, training=training)\n",
    "        shared = self.shared_block(B, training=training)\n",
    "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
    "\n",
    "        for step in self.steps:\n",
    "            entropy_loss += tf.reduce_mean(\n",
    "                tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1))/tf.cast(self.n_steps, tf.float32)\n",
    "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
    "            importance += keys\n",
    "            \n",
    "            shared = self.shared_block(masked, training=training)\n",
    "            split, masked, keys = step(B, shared, prior, training=training)\n",
    "            features = tf.keras.activations.relu(split)\n",
    "            \n",
    "            output += features\n",
    "            encoded += split\n",
    "            \n",
    "        self.add_loss(self.sparsity * entropy_loss)\n",
    "          \n",
    "        prediction = self.final(output)\n",
    "        return prediction, encoded, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=1, \n",
    "                 n_steps = 3, \n",
    "                 n_features = 8,\n",
    "                 outputs = 1, \n",
    "                 gamma = 1.3,\n",
    "                 epsilon = 1e-8, \n",
    "                 sparsity = 1e-5, \n",
    "                 virtual_batch_size=128, \n",
    "                 momentum=0.02):\n",
    "        super(TabNetDecoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = n_features\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def build(self, input_shape: tf.TensorShape):\n",
    "        self.shared_block = FeatureTransformer(\n",
    "            units = self.n_features, \n",
    "            virtual_batch_size=self.virtual_batch_size, \n",
    "            momentum=self.momentum)\n",
    "        self.steps = [\n",
    "            FeatureTransformer(\n",
    "                units = self.n_features,\n",
    "                virtual_batch_size=self.virtual_batch_size, \n",
    "                momentum=self.momentum) for _ in range(self.n_steps)\n",
    "                ]\n",
    "        self.fc = [\n",
    "            tf.keras.layers.Dense(units = self.units) for _ in range(self.n_steps)]\n",
    "    \n",
    "\n",
    "    def call(self, \n",
    "            X: Union[tf.Tensor, np.ndarray], \n",
    "            training: Optional[bool] = None) -> tf.Tensor:\n",
    "        decoded = 0.0\n",
    "        \n",
    "        for ftb, fc in zip(self.steps, self.fc):\n",
    "            shared = self.shared_block(X, training=training)\n",
    "            feature = ftb(shared, training=training)\n",
    "            output = fc(feature)\n",
    "            \n",
    "            decoded += output\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}