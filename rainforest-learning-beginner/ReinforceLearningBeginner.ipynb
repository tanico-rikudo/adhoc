{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforceLearningBeginner",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DluecGi-Za5r"
      },
      "source": [
        "https://www.youtube.com/watch?v=bD6V3rcr_54\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bRuk5kt_LAj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mufD4ndKW_F9"
      },
      "source": [
        "!pip install tensorflow==2.3.0\n",
        "!pip install gym\n",
        "!pip install keras\n",
        "!pip install keras-rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u0m55cdW81f"
      },
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d-tP7eSsuuo"
      },
      "source": [
        "環境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzOGRElKXJrr"
      },
      "source": [
        "# Cutom Env\n",
        "# inherit from openAI Env\n",
        "class ShowEnv(Env):\n",
        "  # initialize\n",
        "  def __init__(self):\n",
        "    # actionの取りうる値の空間\n",
        "    # 0: down, 1: stay, 2: up\n",
        "    self.action_space = Discrete(3)\n",
        "    # 観測データの取りうる値の空間: \n",
        "    # continuous (multiple-value) value.  Used for images dataframe \n",
        "    self.observation_space =  Box(low=np.array([0]), high=np.array([100]))\n",
        "    # state: temprature\n",
        "    self.state = 38+ random.randint(-1,3)\n",
        "    # iteration\n",
        "    self.shower_length = 60\n",
        "\n",
        "  def step(self, action):\n",
        "    # Apply action\n",
        "    # 0 -1 = -1 temperature\n",
        "    # 1 -1 = 0 \n",
        "    # 2 -1 = 1 temperature \n",
        "    self.state += action -1 \n",
        "    # Reduce shower length by 1 second\n",
        "    self.shower_length -= 1 \n",
        "    \n",
        "    # Calculate reward\n",
        "    # very simple\n",
        "    if self.state >=37 and self.state <=39: \n",
        "        reward =1 \n",
        "    else: \n",
        "        reward = -1 \n",
        "    \n",
        "    # Check if shower is done ( check finish )\n",
        "    if self.shower_length <= 0: \n",
        "        done = True\n",
        "    else:\n",
        "        done = False\n",
        "    \n",
        "    # Apply temperature noise\n",
        "    # Havinng nonise is aid to simulate a real env\n",
        "    # but it takes longer \n",
        "    self.state += random.randint(-1,1)\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "    \n",
        "    # Return step information\n",
        "    return self.state, reward, done, info\n",
        "\n",
        "\n",
        "  # visualize\n",
        "  def render(self):\n",
        "    # imfomative\n",
        "    pass\n",
        "\n",
        "  # reset env\n",
        "  def reset(self):\n",
        "      # Reset shower temperature\n",
        "      self.state = 38 + random.randint(-3,3)\n",
        "      # Reset shower time\n",
        "      self.shower_length = 60 \n",
        "      return self.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj7qYysaeuT5"
      },
      "source": [
        "# Get first env\n",
        "env = ShowEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OfoiTCoex0b"
      },
      "source": [
        "# Get first env\n",
        "env = ShowEnv()\n",
        "# temperature example　\n",
        "print(env.observation_space.sample())\n",
        "# Action example\n",
        "print(env.action_space.sample())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxC53c-cfBwr"
      },
      "source": [
        "# 試しに２５回してみる\n",
        "episodes =  25\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    while not done:\n",
        "        #env.render()\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{}'.format(episode, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggCrbrpRfzbM"
      },
      "source": [
        "DQNで用いるDNNモデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbHpIH-Ufhmn"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVEHI0skgUK0"
      },
      "source": [
        "# Create Simple DNN\n",
        "def build_model(states, actions):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(24,activation = 'relu', input_shape=states))\n",
        "  model.add(Dense(24, activation='relu'))\n",
        "  model.add(Dense(actions, activation='linear')) #return action\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xmfz57tg1X1"
      },
      "source": [
        "model = build_model(\n",
        "    states = env.observation_space.shape,\n",
        "    actions = env.action_space.n\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_FLSymhDRq"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhGdMAVjpjeC"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePATL_TGpcX2"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA-I6hXTqAPP"
      },
      "source": [
        "def build_agent(model, actions):\n",
        "  policy = BoltzmannQPolicy()\n",
        "  memory = SequentialMemory(limit=50000, window_length=1)\n",
        "  # DQNのエージェント\n",
        "  # model: 上のDNN\n",
        "  # memory: Experience Replay用のメモリ。特定の順序で呼び出すSequentialMemory\n",
        "  # policy: 行動を選択ルール(方策)。BoltzmannQPolicyは行動のQ値をソフトマックスにかけて選択する\n",
        "\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "  return dqn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVkuRH861bxK"
      },
      "source": [
        "dqn = build_agent(\n",
        "    model = model,\n",
        "    actions = env.action_space.n\n",
        ")\n",
        "dqn.compile(Adam(lr=1e-3),  metrics=['mae'])#lossは設定不可\n",
        "dqn.fit(env, nb_steps=500, visualize=False, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96McsEmt33HR"
      },
      "source": [
        "testする。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwJk87PqKxEE"
      },
      "source": [
        "import rl.callbacks\n",
        "# call back も設定できる\n",
        "class EpisodeLogger(rl.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.observations = {}\n",
        "        self.rewards = {}\n",
        "        self.actions = {}\n",
        "\n",
        "    def on_episode_begin(self, episode, logs):\n",
        "        self.observations[episode] = []\n",
        "        self.rewards[episode] = []\n",
        "        self.actions[episode] = []\n",
        "\n",
        "    def on_step_end(self, step, logs):\n",
        "        episode = logs['episode']\n",
        "        self.observations[episode].append(logs['observation'])\n",
        "        self.rewards[episode].append(logs['reward'])\n",
        "        self.actions[episode].append(logs['action'])\n",
        "cb_ep = EpisodeLogger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5CUjNf31ep-"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epj3CKY0MtSK"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for obs in cb_ep.observations.values():\n",
        "  plt.plot(obs)\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"pos\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYk6tCKkNvxS"
      },
      "source": [
        "# https://qiita.com/goodclues/items/9b2b618ac5ba4c3be1c5#%E7%92%B0%E5%A2%83dnn%E3%83%A2%E3%83%87%E3%83%AB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqoeMdH30hC"
      },
      "source": [
        "#renderを実装すれば行ける\n",
        "# _ = dqn.test(env, nb_episodes=15, visualize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7fLV4KvJcmS"
      },
      "source": [
        "# save\n",
        "dqn.save_weights('dqn_weights.h5f', overwrite=True)\n",
        "# del model\n",
        "# del dqn\n",
        "# del env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srGmKeasJ0CD"
      },
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v0')\n",
        "actions = env.action_space.n\n",
        "states = env.observation_space.shape\n",
        "model = build_model(states, actions)\n",
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.load_weights('dqn_weights.h5f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRLczIVUKB76"
      },
      "source": [
        "_ = dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-19qXfgMrf4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}